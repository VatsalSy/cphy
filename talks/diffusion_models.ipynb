{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d50c687",
   "metadata": {},
   "source": [
    "# Diffusion models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadc1a99",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "<!-- [Click here to open this notebook in Colab](https://colab.research.google.com/github/williamgilpin/cphy/blob/main/talks/svd_decomp.ipynb) -->\n",
    "Open this notebook in Google Colab: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/williamgilpin/cphy/blob/main/talks/svd_decomp.ipynb)\n",
    "\n",
    "We start by importing the necessary Python packages.\n",
    "<!-- *This notebook created by William Gilpin. Consult the [course website](https://www.wgilpin.com/cphy) for all content and [GitHub repository](https://github.com/williamgilpin/cphy) for raw files and runnable online code.* --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ee03d2d-906b-4dee-aee1-b48f2a17a887",
   "metadata": {
    "tags": [
     "remove-cell-pdf"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Wipe all outputs from this notebook\n",
    "from IPython.display import Image, clear_output, display\n",
    "clear_output(True)\n",
    "\n",
    "# Import local plotting functions and in-notebook display functions\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abbebfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# minimal_ddpm_mnist.py\n",
    "import math, torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, utils\n",
    "\n",
    "# ----------------------- diffusion utilities -----------------------\n",
    "\n",
    "\n",
    "def sinusoidal_embedding(t, dim=64):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        t (torch.LongTensor): Timesteps (B,) in [0, T-1].\n",
    "        dim (int): Embedding dimension (even).\n",
    "\n",
    "    Returns:\n",
    "        (torch.Tensor): Positional embeddings (B, dim).\n",
    "    \"\"\"\n",
    "    half = dim // 2\n",
    "    freqs = torch.exp(torch.linspace(math.log(1.0), math.log(10000.0), half, device=t.device))\n",
    "    ang = t[:, None] * freqs[None]\n",
    "    return torch.cat([torch.sin(ang), torch.cos(ang)], dim=1)\n",
    "\n",
    "def q_sample(x0, t, sqrt_alphas_cumprod, sqrt_one_minus_alphas_cumprod, noise=None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x0 (torch.Tensor): Clean images in [-1,1], shape (B,1,H,W).\n",
    "        t (torch.LongTensor): Timesteps (B,).\n",
    "        sqrt_alphas_cumprod (torch.Tensor): Precomputed coefficients (T,).\n",
    "        sqrt_one_minus_alphas_cumprod (torch.Tensor): Precomputed coefficients (T,).\n",
    "        noise (torch.Tensor, optional): Noise to add.\n",
    "\n",
    "    Returns:\n",
    "        (torch.Tensor): Noised x_t with same shape as x0.\n",
    "    \"\"\"\n",
    "    if noise is None: noise = torch.randn_like(x0)\n",
    "    sa = sqrt_alphas_cumprod[t].view(-1,1,1,1)\n",
    "    som = sqrt_one_minus_alphas_cumprod[t].view(-1,1,1,1)\n",
    "    return sa * x0 + som * noise\n",
    "\n",
    "# ----------------------- tiny U-Net (pedagogical) -----------------------\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, ch, tdim):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.GroupNorm(8, ch)\n",
    "        self.conv1 = nn.Conv2d(ch, ch, 3, padding=1)\n",
    "        self.norm2 = nn.GroupNorm(8, ch)\n",
    "        self.conv2 = nn.Conv2d(ch, ch, 3, padding=1)\n",
    "        self.time = nn.Sequential(nn.SiLU(), nn.Linear(tdim, ch))\n",
    "    def forward(self, x, t_emb):\n",
    "        h = self.conv1(F.silu(self.norm1(x)))\n",
    "        h = h + self.time(t_emb).unsqueeze(-1).unsqueeze(-1)\n",
    "        h = self.conv2(F.silu(self.norm2(h)))\n",
    "        return x + h\n",
    "\n",
    "class TinyUNet(nn.Module):\n",
    "    \"\"\"\n",
    "    A very small U-Net for 32x32 grayscale.\n",
    "\n",
    "    Args:\n",
    "        ch (int): Base channel width.\n",
    "        tdim (int): Timestep embedding dim.\n",
    "    \"\"\"\n",
    "    def __init__(self, ch=64, tdim=64):\n",
    "        super().__init__()\n",
    "        self.inp = nn.Conv2d(1, ch, 3, padding=1)\n",
    "        self.rb1 = ResidualBlock(ch, tdim)\n",
    "        self.down = nn.Conv2d(ch, ch*2, 4, stride=2, padding=1)\n",
    "        self.rb2 = ResidualBlock(ch*2, tdim)\n",
    "        self.mid = ResidualBlock(ch*2, tdim)\n",
    "        self.up = nn.ConvTranspose2d(ch*2, ch, 4, stride=2, padding=1)\n",
    "        self.rb3 = ResidualBlock(ch, tdim)\n",
    "        self.out = nn.Conv2d(ch, 1, 3, padding=1)\n",
    "        self.tproj = nn.Sequential(nn.Linear(tdim, tdim*4), nn.SiLU(), nn.Linear(tdim*4, tdim))\n",
    "    def forward(self, x, t):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): Noisy images (B,1,32,32).\n",
    "            t (torch.LongTensor): Timesteps (B,).\n",
    "\n",
    "        Returns:\n",
    "            (torch.Tensor): Predicted noise (B,1,32,32).\n",
    "        \"\"\"\n",
    "        t_emb = self.tproj(sinusoidal_embedding(t, self.tproj[0].in_features))\n",
    "        h1 = self.inp(x); h1 = self.rb1(h1, t_emb)\n",
    "        h2 = self.down(h1); h2 = self.rb2(h2, t_emb)\n",
    "        h  = self.mid(h2, t_emb)\n",
    "        h  = self.up(h); h = h + h1\n",
    "        h  = self.rb3(h, t_emb)\n",
    "        return self.out(h)\n",
    "\n",
    "# ----------------------- loss and sampling -----------------------\n",
    "\n",
    "def diffusion_loss(model, x0, t, sqrt_alphas_cumprod, sqrt_one_minus_alphas_cumprod):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        model (nn.Module): Noise predictor ε_θ(x_t,t).\n",
    "        x0 (torch.Tensor): Clean images in [-1,1], (B,1,H,W).\n",
    "        t (torch.LongTensor): Timesteps (B,).\n",
    "        sqrt_alphas_cumprod (torch.Tensor): Precomputed (T,).\n",
    "        sqrt_one_minus_alphas_cumprod (torch.Tensor): Precomputed (T,).\n",
    "\n",
    "    Returns:\n",
    "        (torch.Tensor): MSE loss between true and predicted noise.\n",
    "    \"\"\"\n",
    "    noise = torch.randn_like(x0)\n",
    "    xt = q_sample(x0, t, sqrt_alphas_cumprod, sqrt_one_minus_alphas_cumprod, noise)\n",
    "    pred = model(xt, t)\n",
    "    return F.mse_loss(pred, noise)\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_ddpm(model, shape, betas, alphas, alphas_cumprod, alphas_cumprod_prev, device):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        model (nn.Module): Trained noise predictor.\n",
    "        shape (tuple): (B,1,32,32).\n",
    "        betas, alphas, alphas_cumprod, alphas_cumprod_prev (torch.Tensor): Precomputed schedules.\n",
    "        device (torch.device): Device.\n",
    "\n",
    "    Returns:\n",
    "        (torch.Tensor): Samples in [-1,1], shape (B,1,32,32).\n",
    "    \"\"\"\n",
    "    T = betas.shape[0]\n",
    "    sqrt_recip_alphas = torch.sqrt(1.0 / alphas).to(device)\n",
    "    sqrt_one_minus_alphas_cumprod = torch.sqrt(1 - alphas_cumprod).to(device)\n",
    "    posterior_variance = betas * (1 - alphas_cumprod_prev) / (1 - alphas_cumprod)\n",
    "\n",
    "    x = torch.randn(shape, device=device)\n",
    "    for t in reversed(range(T)):\n",
    "        tt = torch.full((shape[0],), t, device=device, dtype=torch.long)\n",
    "        eps = model(x, tt)\n",
    "        x0_pred = (x - eps * sqrt_one_minus_alphas_cumprod[t]) / torch.sqrt(alphas_cumprod[t])\n",
    "        coef1 = betas[t] * torch.sqrt(alphas_cumprod_prev[t]) / (1 - alphas_cumprod[t])\n",
    "        coef2 = (torch.sqrt(alphas[t]) * (1 - alphas_cumprod_prev[t])) / (1 - alphas_cumprod[t])\n",
    "        mean = coef1 * x0_pred + coef2 * x\n",
    "        if t > 0:\n",
    "            noise = torch.randn_like(x)\n",
    "            x = mean + torch.sqrt(posterior_variance[t]) * noise\n",
    "        else:\n",
    "            x = mean\n",
    "    return x.clamp(-1, 1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df78e12a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:00<00:00, 31.1MB/s]\n",
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 820kB/s]\n",
      "100%|██████████| 1.65M/1.65M [00:00<00:00, 7.08MB/s]\n",
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 5.34MB/s]\n",
      "/Users/william/mamba/envs/gene/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: loss=0.0518\n",
      "epoch 2: loss=0.0395\n",
      "epoch 3: loss=0.0501\n",
      "epoch 4: loss=0.0406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libc++abi: terminating due to uncaught exception of type std::__1::system_error: Broken pipe\n",
      "libc++abi: terminating due to uncaught exception of type std::__1::system_error: Broken pipe\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x1279d3420>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/william/mamba/envs/gene/lib/python3.13/site-packages/torch/utils/data/dataloader.py\", line 1662, in __del__\n",
      "    def __del__(self):\n",
      "  File \"/Users/william/mamba/envs/gene/lib/python3.13/site-packages/torch/utils/data/_utils/signal_handling.py\", line 73, in handler\n",
      "    _error_if_any_worker_fails()\n",
      "RuntimeError: DataLoader worker (pid 66376) is killed by signal: Abort trap: 6. \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 43\u001b[39m\n\u001b[32m     41\u001b[39m         t = torch.randint(\u001b[32m0\u001b[39m, T, (x.size(\u001b[32m0\u001b[39m),), device=device, dtype=torch.long)\n\u001b[32m     42\u001b[39m         loss = diffusion_loss(model, x, t, sqrt_alphas_cumprod, sqrt_one_minus_alphas_cumprod)\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m         opt.zero_grad(); loss.backward(); opt.step()\n\u001b[32m     44\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mepoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss.item()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     46\u001b[39m torch.save(model.state_dict(), \u001b[33m\"\u001b[39m\u001b[33mckpt.pt\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mamba/envs/gene/lib/python3.13/site-packages/torch/_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mamba/envs/gene/lib/python3.13/site-packages/torch/autograd/__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mamba/envs/gene/lib/python3.13/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ----------------------- training script -----------------------\n",
    "\n",
    "# def main():\n",
    "#     \"\"\"\n",
    "#     Minimal training on MNIST (resized to 32x32) and sampling a grid.\n",
    "\n",
    "#     Saves:\n",
    "#         samples.png: A 8x8 grid of generated digits.\n",
    "#         ckpt.pt: Model checkpoint after training.\n",
    "#     \"\"\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# data\n",
    "tfm = transforms.Compose([transforms.Resize(32), transforms.ToTensor(), transforms.Normalize(0.5, 0.5)])\n",
    "ds = datasets.MNIST(root='./data', train=True, download=True, transform=tfm)\n",
    "dl = torch.utils.data.DataLoader(ds, batch_size=128, shuffle=True, num_workers=2, pin_memory=True)\n",
    "\n",
    "# schedule (shorter T for pedagogy)\n",
    "T = 300\n",
    "# betas = linear_beta_schedule(T).to(device)\n",
    "beta_start=1e-4\n",
    "beta_end=0.02\n",
    "betas = torch.linspace(beta_start, beta_end, T).to(device) # linear schedule\n",
    "alphas = (1.0 - betas)\n",
    "alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "alphas_cumprod_prev = torch.cat([torch.ones(1, device=device), alphas_cumprod[:-1]])\n",
    "\n",
    "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
    "sqrt_one_minus_alphas_cumprod = torch.sqrt(1 - alphas_cumprod)\n",
    "\n",
    "# model & optim\n",
    "model = TinyUNet(ch=64, tdim=64).to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=2e-4)\n",
    "\n",
    "# train (few epochs; adjust as desired)\n",
    "model.train()\n",
    "for epoch in range(5):\n",
    "    for x,_ in dl:\n",
    "        x = x.to(device)  # already in [-1,1] via Normalize(0.5,0.5)\n",
    "        t = torch.randint(0, T, (x.size(0),), device=device, dtype=torch.long)\n",
    "        loss = diffusion_loss(model, x, t, sqrt_alphas_cumprod, sqrt_one_minus_alphas_cumprod)\n",
    "        opt.zero_grad(); loss.backward(); opt.step()\n",
    "    print(f\"epoch {epoch+1}: loss={loss.item():.4f}\")\n",
    "\n",
    "torch.save(model.state_dict(), \"ckpt.pt\")\n",
    "\n",
    "# sample and save\n",
    "model.eval()\n",
    "samples = sample_ddpm(model, (64,1,32,32), betas, alphas, alphas_cumprod, alphas_cumprod_prev, device)\n",
    "# utils.save_image((samples+1)/2, \"samples.png\", nrow=8)  # back to [0,1]\n",
    "# print(\"wrote samples.png\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd9a002",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "113c72e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimal DDPM on MNIST (pure PyTorch, ~120 lines)\n",
    "import math, torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, utils\n",
    "\n",
    "# ----------------------- tiny UNet -----------------------\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, c_in, c_out):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(c_in, c_out, 3, padding=1), nn.GroupNorm(8, c_out), nn.SiLU(),\n",
    "            nn.Conv2d(c_out, c_out, 3, padding=1), nn.GroupNorm(8, c_out), nn.SiLU(),\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "class TinyUNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        n_steps (int): Number of diffusion steps for sinusoidal time embedding.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_steps=200):\n",
    "        super().__init__()\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            nn.Linear(64, 128), nn.SiLU(),\n",
    "            nn.Linear(128, 128),\n",
    "        )\n",
    "        def tembed(t, dim=64):\n",
    "            half = dim//2\n",
    "            freqs = torch.exp(torch.linspace(0, math.log(10000), half, device=t.device))\n",
    "            args = t[:,None]/n_steps\n",
    "            emb = torch.cat([torch.sin(args*freqs), torch.cos(args*freqs)], dim=-1)\n",
    "            return emb\n",
    "        self.tembed = tembed\n",
    "\n",
    "        self.inp  = nn.Conv2d(1, 32, 3, padding=1)\n",
    "        self.b1   = Block(32, 64)\n",
    "        self.down = nn.Conv2d(64, 64, 4, 2, 1)\n",
    "        self.b2   = Block(64, 128)\n",
    "        self.mid  = Block(128, 128)\n",
    "        self.up   = nn.ConvTranspose2d(128, 64, 4, 2, 1)\n",
    "        self.b3   = Block(64+64, 64)\n",
    "        self.out  = nn.Conv2d(64, 1, 1)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): Noisy images (N,1,28,28).\n",
    "            t (Tensor): Integer timesteps (N,).\n",
    "        Returns:\n",
    "            Tensor: Predicted noise ε_θ(x_t,t) with same shape as x.\n",
    "        \"\"\"\n",
    "        temb = self.time_mlp(self.tembed(t))[:, :, None, None]\n",
    "        h0 = F.silu(self.inp(x))\n",
    "        h1 = self.b1(h0 + temb)\n",
    "        h2 = self.down(h1)\n",
    "        h3 = self.b2(h2 + temb)\n",
    "        m  = self.mid(h3 + temb)\n",
    "        u  = self.up(m)\n",
    "        u  = self.b3(torch.cat([u, h1], 1) + temb)\n",
    "        return self.out(u)\n",
    "\n",
    "# ----------------------- diffusion utils -----------------------\n",
    "class Diffusion:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        n_steps (int): Number of diffusion steps.\n",
    "        beta_start (float): Start of linear beta schedule.\n",
    "        beta_end (float): End of linear beta schedule.\n",
    "        device (str): 'cuda' or 'cpu'.\n",
    "    Attributes:\n",
    "        betas (Tensor): β_t schedule.\n",
    "        alphas_cumprod (Tensor): ∏_{s<=t} (1-β_s).\n",
    "        alphas_cumprod_prev (Tensor): ∏_{s<=t-1} (1-β_s).\n",
    "        sqrt_alphas_cumprod (Tensor)\n",
    "        sqrt_one_minus_alphas_cumprod (Tensor)\n",
    "        posterior_variance (Tensor): q(x_{t-1}|x_t,x_0) variance.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_steps=200, beta_start=1e-4, beta_end=0.02, device=\"cpu\"):\n",
    "        self.device = device\n",
    "        self.n_steps = n_steps\n",
    "        self.betas = torch.linspace(beta_start, beta_end, n_steps, device=device)\n",
    "        alphas = 1.0 - self.betas\n",
    "        self.alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "        self.alphas_cumprod_prev = torch.cat([torch.tensor([1.0], device=device), self.alphas_cumprod[:-1]])\n",
    "        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)\n",
    "        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - self.alphas_cumprod)\n",
    "        self.posterior_variance = self.betas * (1.0 - self.alphas_cumprod_prev) / (1.0 - self.alphas_cumprod)\n",
    "\n",
    "    def q_sample(self, x0, t, noise=None):\n",
    "        \"\"\"Forward diffusion q(x_t|x_0): add noise at step t.\"\"\"\n",
    "        if noise is None: noise = torch.randn_like(x0)\n",
    "        s1 = self.index(self.sqrt_alphas_cumprod, t, x0.shape)\n",
    "        s2 = self.index(self.sqrt_one_minus_alphas_cumprod, t, x0.shape)\n",
    "        return s1 * x0 + s2 * noise\n",
    "\n",
    "    def p_sample(self, model, x, t):\n",
    "        \"\"\"Single reverse step using ε-prediction parameterization.\"\"\"\n",
    "        betat = self.index(self.betas, t, x.shape)\n",
    "        ac_t  = self.index(self.alphas_cumprod, t, x.shape)\n",
    "        sqrt_one_minus_ac_t = self.index(self.sqrt_one_minus_alphas_cumprod, t, x.shape)\n",
    "        eps = model(x, t)\n",
    "        x0_hat = (x - sqrt_one_minus_ac_t * eps) / torch.sqrt(ac_t)\n",
    "        mean = (1/torch.sqrt(1 - betat))*(x - betat/torch.sqrt(1 - ac_t) * eps)\n",
    "        var = self.index(self.posterior_variance, t, x.shape)\n",
    "        if (t == 0).all(): return mean\n",
    "        return mean + torch.sqrt(var) * torch.randn_like(x)\n",
    "\n",
    "    def sample(self, model, n, shape):\n",
    "        \"\"\"Draw n samples by iterating t=T-1..0.\"\"\"\n",
    "        model.eval()\n",
    "        x = torch.randn(n, *shape, device=self.device)\n",
    "        for t in reversed(range(self.n_steps)):\n",
    "            tt = torch.full((n,), t, device=self.device, dtype=torch.long)\n",
    "            with torch.no_grad():\n",
    "                x = self.p_sample(model, x, tt)\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def index(a, t, x_shape):\n",
    "        return a.gather(-1, t).reshape(-1, *([1]*(len(x_shape)-1)))\n",
    "\n",
    "# # ----------------------- training script -----------------------\n",
    "# def main():\n",
    "#     \"\"\"\n",
    "#     Minimal training loop. Produces samples in ./ddpm_samples.png.\n",
    "\n",
    "#     Returns:\n",
    "#         None\n",
    "#     \"\"\"\n",
    "#     device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#     T = 200\n",
    "#     bs = 128\n",
    "#     epochs = 1  # bump to 5–10 for better quality\n",
    "#     lr = 2e-4\n",
    "\n",
    "#     tfm = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda x: x*2-1)])  # [-1,1]\n",
    "#     ds  = datasets.MNIST(root=\"./data\", train=True, download=True, transform=tfm)\n",
    "#     dl  = DataLoader(ds, batch_size=bs, shuffle=True, num_workers=2, drop_last=True)\n",
    "\n",
    "#     net = TinyUNet(n_steps=T).to(device)\n",
    "#     diff = Diffusion(n_steps=T, device=device)\n",
    "#     opt = torch.optim.AdamW(net.parameters(), lr=lr)\n",
    "\n",
    "#     for epoch in range(epochs):\n",
    "#         for x,_ in dl:\n",
    "#             x = x.to(device)\n",
    "#             t = torch.randint(0, T, (x.size(0),), device=device).long()\n",
    "#             noise = torch.randn_like(x)\n",
    "#             x_t = diff.q_sample(x, t, noise)\n",
    "#             pred = net(x_t, t)\n",
    "#             loss = F.mse_loss(pred, noise)\n",
    "#             opt.zero_grad(); loss.backward(); opt.step()\n",
    "#         print(f\"epoch {epoch+1} | loss={loss.item():.4f}\")\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         imgs = diff.sample(net, n=64, shape=(1,28,28))\n",
    "#         imgs = (imgs.clamp(-1,1)+1)/2\n",
    "#     return imgs\n",
    "# #         utils.save_image(imgs, \"ddpm_samples.png\", nrow=8)\n",
    "# #     print(\"Wrote ddpm_samples.png\")\n",
    "\n",
    "# # if __name__ == \"__main__\":\n",
    "# #     main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134a1ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:00<00:00, 23.2MB/s]\n",
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 722kB/s]\n",
      "100%|██████████| 1.65M/1.65M [00:00<00:00, 8.15MB/s]\n",
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 4.94MB/s]\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Can't pickle <function <lambda> at 0x34fd898a0>: attribute lookup <lambda> on __main__ failed",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPicklingError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m opt = torch.optim.AdamW(net.parameters(), lr=lr)\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdl\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrandint\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlong\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mamba/envs/gene/lib/python3.13/site-packages/torch/utils/data/dataloader.py:493\u001b[39m, in \u001b[36mDataLoader.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    491\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._iterator\n\u001b[32m    492\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m493\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mamba/envs/gene/lib/python3.13/site-packages/torch/utils/data/dataloader.py:424\u001b[39m, in \u001b[36mDataLoader._get_iterator\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    422\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    423\u001b[39m     \u001b[38;5;28mself\u001b[39m.check_worker_number_rationality()\n\u001b[32m--> \u001b[39m\u001b[32m424\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mamba/envs/gene/lib/python3.13/site-packages/torch/utils/data/dataloader.py:1171\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter.__init__\u001b[39m\u001b[34m(self, loader)\u001b[39m\n\u001b[32m   1164\u001b[39m w.daemon = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1165\u001b[39m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[32m   1166\u001b[39m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[32m   1167\u001b[39m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[32m   1168\u001b[39m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[32m   1169\u001b[39m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[32m   1170\u001b[39m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1171\u001b[39m \u001b[43mw\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1172\u001b[39m \u001b[38;5;28mself\u001b[39m._index_queues.append(index_queue)\n\u001b[32m   1173\u001b[39m \u001b[38;5;28mself\u001b[39m._workers.append(w)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mamba/envs/gene/lib/python3.13/multiprocessing/process.py:121\u001b[39m, in \u001b[36mBaseProcess.start\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process._config.get(\u001b[33m'\u001b[39m\u001b[33mdaemon\u001b[39m\u001b[33m'\u001b[39m), \\\n\u001b[32m    119\u001b[39m        \u001b[33m'\u001b[39m\u001b[33mdaemonic processes are not allowed to have children\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    120\u001b[39m _cleanup()\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m \u001b[38;5;28mself\u001b[39m._popen = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[38;5;28mself\u001b[39m._sentinel = \u001b[38;5;28mself\u001b[39m._popen.sentinel\n\u001b[32m    123\u001b[39m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[32m    124\u001b[39m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mamba/envs/gene/lib/python3.13/multiprocessing/context.py:224\u001b[39m, in \u001b[36mProcess._Popen\u001b[39m\u001b[34m(process_obj)\u001b[39m\n\u001b[32m    222\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    223\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_Popen\u001b[39m(process_obj):\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mProcess\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mamba/envs/gene/lib/python3.13/multiprocessing/context.py:289\u001b[39m, in \u001b[36mSpawnProcess._Popen\u001b[39m\u001b[34m(process_obj)\u001b[39m\n\u001b[32m    286\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_Popen\u001b[39m(process_obj):\n\u001b[32m    288\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpopen_spawn_posix\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[32m--> \u001b[39m\u001b[32m289\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mamba/envs/gene/lib/python3.13/multiprocessing/popen_spawn_posix.py:32\u001b[39m, in \u001b[36mPopen.__init__\u001b[39m\u001b[34m(self, process_obj)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, process_obj):\n\u001b[32m     31\u001b[39m     \u001b[38;5;28mself\u001b[39m._fds = []\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mamba/envs/gene/lib/python3.13/multiprocessing/popen_fork.py:20\u001b[39m, in \u001b[36mPopen.__init__\u001b[39m\u001b[34m(self, process_obj)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28mself\u001b[39m.returncode = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28mself\u001b[39m.finalizer = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_launch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mamba/envs/gene/lib/python3.13/multiprocessing/popen_spawn_posix.py:47\u001b[39m, in \u001b[36mPopen._launch\u001b[39m\u001b[34m(self, process_obj)\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     46\u001b[39m     reduction.dump(prep_data, fp)\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     \u001b[43mreduction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     49\u001b[39m     set_spawning_popen(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mamba/envs/gene/lib/python3.13/multiprocessing/reduction.py:60\u001b[39m, in \u001b[36mdump\u001b[39m\u001b[34m(obj, file, protocol)\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdump\u001b[39m(obj, file, protocol=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     59\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m     \u001b[43mForkingPickler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mPicklingError\u001b[39m: Can't pickle <function <lambda> at 0x34fd898a0>: attribute lookup <lambda> on __main__ failed"
     ]
    }
   ],
   "source": [
    "T = 200 # number of diffusion steps\n",
    "bs = 128 # batch size \n",
    "epochs = 1  # bump to 5–10 for better quality\n",
    "lr = 2e-4 # learning rate\n",
    "## If torch is able to find a GPU, use it. Otherwise, use the CPU.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "ds  = datasets.MNIST(root=\"./\", train=True, download=True, transform=tfm)\n",
    "dl  = DataLoader(ds, batch_size=bs, shuffle=True, num_workers=2, drop_last=True)\n",
    "\n",
    "net = TinyUNet(n_steps=T).to(device)\n",
    "diff = Diffusion(n_steps=T, device=device)\n",
    "opt = torch.optim.AdamW(net.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for x,_ in dl:\n",
    "        x = x.to(device)\n",
    "        t = torch.randint(0, T, (x.size(0),), device=device).long()\n",
    "        noise = torch.randn_like(x)\n",
    "        x_t = diff.q_sample(x, t, noise)\n",
    "        pred = net(x_t, t)\n",
    "        loss = F.mse_loss(pred, noise)\n",
    "        opt.zero_grad(); loss.backward(); opt.step()\n",
    "    print(f\"epoch {epoch+1} | loss={loss.item():.4f}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    imgs = diff.sample(net, n=64, shape=(1,28,28))\n",
    "    imgs = (imgs.clamp(-1,1)+1)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f3ccda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e574d7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d81934",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc395c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75293f38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c330949c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a17281",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
